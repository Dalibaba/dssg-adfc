{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce37d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumwire import webdriver\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ea851",
   "metadata": {},
   "source": [
    "# Get metadata of all bike counting stations of a certain city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "#found by googling https://data.eco-counter.com/ParcPublic/?id=\n",
    "ids = {\n",
    "    'Stuttgart': 607,\n",
    "    'Rostock': 888, \n",
    "    'Berlin': 4728, \n",
    "    'Bochum': 6603,\n",
    "    'Düsseldorf': 857,\n",
    "    'Köln': 677,\n",
    "    'Reutlingen': 7581\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ids.items():\n",
    "    city = k\n",
    "    url = f\"https://data.eco-counter.com/ParcPublic/?id={v}#\"\n",
    "    # get driver path\n",
    "    current_path = pathlib.Path().resolve()\n",
    "    driver_path = os.path.join(current_path, \"chromedriver\")\n",
    "\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # wait for page to load competletely\n",
    "    time.sleep(1)\n",
    "    # get all requests made by the page\n",
    "    all_requests = driver.requests\n",
    "    \n",
    "    # define target url, which shall be filtered out\n",
    "    TARGET_URL = \"www.eco-visio.net\"\n",
    "    GET_REQUEST = \"\"\n",
    "    \n",
    "    for request in all_requests:\n",
    "        # parse url and filter out query param\n",
    "        parsed_url = urlparse(request.url)\n",
    "        query_dictionary = parse_qs(parsed_url.query)\n",
    "        # check if target url matches\n",
    "        if (parsed_url.netloc == TARGET_URL):\n",
    "            GET_REQUEST = request\n",
    "            print(GET_REQUEST.url)\n",
    "            if GET_REQUEST:\n",
    "#                break\n",
    "            \n",
    "                url_endpoint = GET_REQUEST\n",
    "                response = requests.get(url_endpoint)\n",
    "                response\n",
    "\n",
    "                data_json = json.loads(response.content)\n",
    "                df_big = pd.DataFrame(data_json)\n",
    "                dfs.append(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-theme",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_big.loc[(df_big['lon'] < 16) & (df_big['lon'] > 6) & (df_big['lat'] < 55) & (df_big['lat'] > 47)][['id_pdc_img', 'idPdc','lienPublic', 'nom', 'lat', 'lon', 'total', 'nomOrganisme']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-practice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30440e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_STUTTGART = 'https://data.eco-counter.com/ParcPublic/?id=607#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get driver path\n",
    "current_path = pathlib.Path().resolve()\n",
    "driver_path = os.path.join(current_path, \"chromedriver\")\n",
    "\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "driver.get(URL_STUTTGART)\n",
    "\n",
    "# wait for page to load competletely\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all requests made by the page\n",
    "all_requests = driver.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target url, which shall be filtered out\n",
    "TARGET_URL = \"www.eco-visio.net\"\n",
    "GET_REQUEST = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c34be",
   "metadata": {},
   "source": [
    "Filter out the get request made to the database and get the metadata of all bike counting stations in a city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for request in all_requests:\n",
    "    # parse url and filter out query param\n",
    "    parsed_url = urlparse(request.url)\n",
    "    query_dictionary = parse_qs(parsed_url.query)\n",
    "    # check if target url matches\n",
    "    if (parsed_url.netloc == TARGET_URL):\n",
    "        GET_REQUEST = request\n",
    "        print(GET_REQUEST.url)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf29c11",
   "metadata": {},
   "source": [
    "Fetch metadata information of all counting stations of a certain city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_endpoint = GET_REQUEST\n",
    "response = requests.get(url_endpoint)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1d26c",
   "metadata": {},
   "source": [
    "extract relevant metadata of the counting stations and save it to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_station_list = []\n",
    "for counting_station in data_json:\n",
    "    counting_station_dict = {}\n",
    "    counting_station_dict[\"id\"] = counting_station[\"idPdc\"]\n",
    "    counting_station_dict[\"name\"] = counting_station[\"nom\"]\n",
    "    counting_station_dict[\"latitude\"] = counting_station[\"lat\"]\n",
    "    counting_station_dict[\"longitude\"] = counting_station[\"lon\"]\n",
    "    counting_station_dict[\"start_time\"] = counting_station[\"debut\"]\n",
    "    counting_station_list.append(counting_station_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['Stuttgart'] = counting_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metadata/counting_stations_metadata.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328d646",
   "metadata": {},
   "source": [
    "# Get data from bike counting stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations = pd.read_csv('additional_metadata.csv') \n",
    "df_metadata_counting_stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations['number'] = df_metadata_counting_stations.groupby('name').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row = df_metadata_counting_stations.loc[df_metadata_counting_stations['id_list'] == 100055269]\n",
    "print(df_row[\"name\"].values[0])\n",
    "df_row[\"number\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of all stations\n",
    "counting_station_ids = list(df_metadata_counting_stations[\"id_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9384f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN = 20160101\n",
    "END = 20210930\n",
    "STEP = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282df634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL for specific bike counting station\n",
    "\n",
    "def get_data_api_request(counting_station_id):\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "    # define target url, which shall be filtered out\n",
    "    TARGET_URL = \"www.eco-visio.net\"\n",
    "    target_get_request = \"\"\n",
    "    BASE_URL = \"https://data.eco-counter.com/public2/?id=\" +  str(counting_station_id)\n",
    "    driver.get(BASE_URL)\n",
    "    print(BASE_URL)\n",
    "    # wait for page to load competletely\n",
    "    time.sleep(1)\n",
    "\n",
    "    # get all requests made by the page\n",
    "    all_requests = driver.requests\n",
    "    \n",
    "    for request in all_requests:\n",
    "        # parse url and filter out query param\n",
    "        parsed_url = urlparse(request.url)\n",
    "        query_dictionary = parse_qs(parsed_url.query)\n",
    "        # check if target url matches and has a token in query\n",
    "        if (parsed_url.netloc == TARGET_URL and \"t\" in query_dictionary):\n",
    "            target_get_request = request\n",
    "            break\n",
    "            \n",
    "            \n",
    "    driver.close()\n",
    "    return target_get_request\n",
    "\n",
    "# Make Request and fetch the data\n",
    "    \n",
    "def create_data_api_request(get_request):\n",
    "    parsed_url = urlparse(get_request.url)\n",
    "    query_dictionary = parse_qs(parsed_url.query)\n",
    "    \n",
    "    query_dictionary = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Define Query Parameters for fetching the Data\n",
    "    \n",
    "    query_dictionary[\"begin\"] = BEGIN\n",
    "    query_dictionary[\"end\"] = END\n",
    "    query_dictionary[\"step\"] = STEP\n",
    "    \n",
    "    url_endpoint = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path\n",
    "    return url_endpoint, query_dictionary\n",
    "\n",
    "# Convert fetched data stream to csv\n",
    "\n",
    "def save_data_to_csv(data_json, counting_station_id):\n",
    "    keys = data_json[0].keys()\n",
    "    date_list = []\n",
    "    comptage_list = []\n",
    "    timestamp_list = []   \n",
    "    columns = [\"date\", \"comptage\", \"timestamp\"]\n",
    "    for data_object in data_json:\n",
    "        date_list.append(data_object[\"date\"])\n",
    "        comptage_list.append(data_object[\"comptage\"])\n",
    "        timestamp_list.append(data_object[\"timestamp\"])    \n",
    "    # Calling DataFrame constructor after zipping\n",
    "    # both lists, with columns specified\n",
    "    df = pd.DataFrame(list(zip(date_list, comptage_list,timestamp_list)),\n",
    "                   columns =columns)  \n",
    "    \n",
    "    #create name\n",
    "    df_row = df_metadata_counting_stations.loc[df_metadata_counting_stations['id_list'] == counting_station_id]  \n",
    "    file_name = df_row[\"name\"].values[0] + \"_\" + str(df_row[\"number\"].values[0]) + \".csv\"\n",
    "    \n",
    "    path = os.path.join(\"data\",file_name )\n",
    "    \n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_request = get_data_api_request(100030582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-durham",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_data_list = []\n",
    "\n",
    "for counting_station_id in counting_station_ids:\n",
    "    \n",
    "    get_request = get_data_api_request(counting_station_id)\n",
    "    if get_request != \"\":\n",
    "        \n",
    "        url_endpoint, query_dictionary = create_data_api_request(get_request)\n",
    "        response = requests.get(url_endpoint, params=query_dictionary)\n",
    "        print(response)\n",
    "        \n",
    "        data_json = json.loads(response.content)\n",
    "        save_data_to_csv(data_json, counting_station_id)\n",
    "        \n",
    "        found_data_list.append(1)\n",
    "    else:\n",
    "        \n",
    "        found_data_list.append(0)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58547d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations['fetched data ?'] = found_data_list\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-lighting",
   "metadata": {},
   "source": [
    "# Berlin special case (outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_endpoint = GET_REQUEST\n",
    "\n",
    "BERLIN_URL = \"https://www.eco-visio.net/api/aladdin/1.0.0/pbl/publicwebpageplus/4728?withNull=true\"\n",
    "url_endpoint = BERLIN_URL\n",
    "response = requests.get(url_endpoint)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_json = json.loads(response.content)\n",
    "df_berlin = pd.DataFrame(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weird column ordering below to get the existing format - sorry\n",
    "df = df_berlin[['idPdc', 'lat', 'lon', 'nom']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = 'Berlin - ' + df['nom'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[['name', 'lat', 'lon', 'nom', 'idPdc']].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns = ['number', 'name', 'lat', 'lon', 'nom', 'id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test[['name', 'number', 'lat', 'lon', 'id_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number'] = df['number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_counting_stations = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of all stations\n",
    "counting_station_ids = list(df_metadata_counting_stations[\"id_list\"])\n",
    "BEGIN = 20160101\n",
    "END = 20210926\n",
    "STEP = 2\n",
    "\n",
    "found_data_list = []\n",
    "\n",
    "for counting_station_id in counting_station_ids:\n",
    "    \n",
    "    get_request = get_data_api_request(counting_station_id)\n",
    "    if get_request != \"\":\n",
    "        \n",
    "        url_endpoint, query_dictionary = create_data_api_request(get_request)\n",
    "        response = requests.get(url_endpoint, params=query_dictionary)\n",
    "        print(response)\n",
    "        \n",
    "        data_json = json.loads(response.content)\n",
    "        save_data_to_csv(data_json, counting_station_id)\n",
    "        \n",
    "        found_data_list.append(1)\n",
    "    else:\n",
    "        \n",
    "        found_data_list.append(0)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(found_data_list)\n",
    "\n",
    "df_metadata_counting_stations['fetched data ?'] = found_data_list\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('berlin_stations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-triple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
